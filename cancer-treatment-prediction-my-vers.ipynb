{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6841,"databundleVersionId":44307,"sourceType":"competition"},{"sourceId":5893482,"sourceType":"datasetVersion","datasetId":3385928}],"dockerImageVersionId":30474,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1><center>Cancer treatment prediction </center></h1>","metadata":{}},{"cell_type":"markdown","source":"kaggle competitions download -c msk-redefining-cancer-treatment","metadata":{}},{"cell_type":"markdown","source":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:lightblue; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Quick navigation</center></h2>\n\n* [0. Installation of libraries](#0)\n* [1. Basic Data Overview](#1)\n* [2. Analysis of data](#2)\n* [3. EDA](#3)\n* [4. Classification models](#4)","metadata":{}},{"cell_type":"markdown","source":"\n<a id=\"0\"></a>\n<h2 style='background:lightblue; border:0; color:white'><center>0. ðŸ“š Installation of libraries</center><h2>","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom  sklearn.ensemble import IsolationForest\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.preprocessing  import LabelEncoder\nfrom sklearn import linear_model \nfrom sklearn import tree \nfrom sklearn import ensemble \nfrom sklearn import metrics \nfrom sklearn import preprocessing \nfrom sklearn.model_selection import train_test_split \nfrom sklearn.feature_selection import SelectKBest, f_classif\nimport plotly \nimport plotly.express as px\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import log_loss, accuracy_score\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import SVC\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import LabelEncoder\n\nimport gensim\n\nimport scikitplot.plotters as skplt\n\nimport nltk\n\nfrom xgboost import XGBClassifier\n\nimport os\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding, LSTM\nfrom keras.utils.np_utils import to_categorical\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.models import load_model\nfrom keras.optimizers import Adam\n\nimport zipfile","metadata":{"execution":{"iopub.status.busy":"2023-12-18T09:38:57.315985Z","iopub.execute_input":"2023-12-18T09:38:57.316798Z","iopub.status.idle":"2023-12-18T09:39:06.426355Z","shell.execute_reply.started":"2023-12-18T09:38:57.316762Z","shell.execute_reply":"2023-12-18T09:39:06.425465Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a id=\"1\"></a>\n<h2 style='background:lightblue; border:0; color:white'><center>1. Basic Data Overview</center><h2>","metadata":{}},{"cell_type":"markdown","source":"develop a Machine Learning algorithm that, using this knowledge base as a baseline, automatically classifies genetic variations.","metadata":{}},{"cell_type":"markdown","source":"**training_variants** - a comma separated file containing the description of the genetic mutations used for training. Fields are ID (the id of the row used to link the mutation to the clinical evidence), Gene (the gene where this genetic mutation is located), Variation (the aminoacid change for this mutations), Class (1-9 the class this genetic mutation has been classified on)\n\n**training_text** - a double pipe (||) delimited file that contains the clinical evidence (text) used to classify genetic mutations. Fields are ID (the id of the row used to link the clinical evidence to the genetic mutation), Text (the clinical evidence used to classify the genetic mutation)\n\n**test_variants** - a comma separated file containing the description of the genetic mutations used for training. Fields are ID (the id of the row used to link the mutation to the clinical evidence), Gene (the gene where this genetic mutation is located), Variation (the aminoacid change for this mutations)\n\n**test_text** - a double pipe (||) delimited file that contains the clinical evidence (text) used to classify genetic mutations. Fields are ID (the id of the row used to link the clinical evidence to the genetic mutation), Text (the clinical evidence used to classify the genetic mutation)\n\nsubmissionSample - a sample submission file:\n\nFor each ID in the test set, you must predict a probability for each of the different classes a genetic mutation can be classified on. The file should contain a header and have the following format:\n\nID,class1,class2,class3,class4,class5,class6,class7,class8,class9","metadata":{}},{"cell_type":"code","source":"train_text = pd.read_csv('/kaggle/input/unzip-files/training_text',\n                         sep='\\|\\|', \n                         header=None, \n                         skiprows=1, \n                         names=[\"ID\",\"Text\"])\ndisplay(train_text.info(),\n       train_text.head())","metadata":{"execution":{"iopub.status.busy":"2023-12-18T09:39:06.428506Z","iopub.execute_input":"2023-12-18T09:39:06.429271Z","iopub.status.idle":"2023-12-18T09:39:09.540142Z","shell.execute_reply.started":"2023-12-18T09:39:06.429241Z","shell.execute_reply":"2023-12-18T09:39:09.539126Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 3321 entries, 0 to 3320\nData columns (total 2 columns):\n #   Column  Non-Null Count  Dtype \n---  ------  --------------  ----- \n 0   ID      3321 non-null   int64 \n 1   Text    3316 non-null   object\ndtypes: int64(1), object(1)\nmemory usage: 52.0+ KB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"None"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"   ID                                               Text\n0   0  Cyclin-dependent kinases (CDKs) regulate a var...\n1   1   Abstract Background  Non-small cell lung canc...\n2   2   Abstract Background  Non-small cell lung canc...\n3   3  Recent evidence has demonstrated that acquired...\n4   4  Oncogenic mutations in the monomeric Casitas B...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>Text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>Cyclin-dependent kinases (CDKs) regulate a var...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>Abstract Background  Non-small cell lung canc...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>Abstract Background  Non-small cell lung canc...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>Recent evidence has demonstrated that acquired...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>Oncogenic mutations in the monomeric Casitas B...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train_var = pd.read_csv('/kaggle/input/unzip-files/training_variants')\ndisplay(train_var.info(),\n       train_var.head(),\n       train_var.describe())","metadata":{"execution":{"iopub.status.busy":"2023-12-18T09:39:09.541444Z","iopub.execute_input":"2023-12-18T09:39:09.541778Z","iopub.status.idle":"2023-12-18T09:39:09.588913Z","shell.execute_reply.started":"2023-12-18T09:39:09.541747Z","shell.execute_reply":"2023-12-18T09:39:09.587824Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 3321 entries, 0 to 3320\nData columns (total 4 columns):\n #   Column     Non-Null Count  Dtype \n---  ------     --------------  ----- \n 0   ID         3321 non-null   int64 \n 1   Gene       3321 non-null   object\n 2   Variation  3321 non-null   object\n 3   Class      3321 non-null   int64 \ndtypes: int64(2), object(2)\nmemory usage: 103.9+ KB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"None"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"   ID    Gene             Variation  Class\n0   0  FAM58A  Truncating Mutations      1\n1   1     CBL                 W802*      2\n2   2     CBL                 Q249E      2\n3   3     CBL                 N454D      3\n4   4     CBL                 L399V      4","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>Gene</th>\n      <th>Variation</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>FAM58A</td>\n      <td>Truncating Mutations</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>CBL</td>\n      <td>W802*</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>CBL</td>\n      <td>Q249E</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>CBL</td>\n      <td>N454D</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>CBL</td>\n      <td>L399V</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"                ID        Class\ncount  3321.000000  3321.000000\nmean   1660.000000     4.365854\nstd     958.834449     2.309781\nmin       0.000000     1.000000\n25%     830.000000     2.000000\n50%    1660.000000     4.000000\n75%    2490.000000     7.000000\nmax    3320.000000     9.000000","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>3321.000000</td>\n      <td>3321.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>1660.000000</td>\n      <td>4.365854</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>958.834449</td>\n      <td>2.309781</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>830.000000</td>\n      <td>2.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>1660.000000</td>\n      <td>4.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>2490.000000</td>\n      <td>7.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>3320.000000</td>\n      <td>9.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train = train_var.merge(train_text, how='left', on='ID')\ndisplay(train.info(),\n       train.head()\n       )","metadata":{"execution":{"iopub.status.busy":"2023-12-18T09:39:09.590238Z","iopub.execute_input":"2023-12-18T09:39:09.590576Z","iopub.status.idle":"2023-12-18T09:39:09.626327Z","shell.execute_reply.started":"2023-12-18T09:39:09.590548Z","shell.execute_reply":"2023-12-18T09:39:09.625282Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nInt64Index: 3321 entries, 0 to 3320\nData columns (total 5 columns):\n #   Column     Non-Null Count  Dtype \n---  ------     --------------  ----- \n 0   ID         3321 non-null   int64 \n 1   Gene       3321 non-null   object\n 2   Variation  3321 non-null   object\n 3   Class      3321 non-null   int64 \n 4   Text       3316 non-null   object\ndtypes: int64(2), object(3)\nmemory usage: 155.7+ KB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"None"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"   ID    Gene             Variation  Class  \\\n0   0  FAM58A  Truncating Mutations      1   \n1   1     CBL                 W802*      2   \n2   2     CBL                 Q249E      2   \n3   3     CBL                 N454D      3   \n4   4     CBL                 L399V      4   \n\n                                                Text  \n0  Cyclin-dependent kinases (CDKs) regulate a var...  \n1   Abstract Background  Non-small cell lung canc...  \n2   Abstract Background  Non-small cell lung canc...  \n3  Recent evidence has demonstrated that acquired...  \n4  Oncogenic mutations in the monomeric Casitas B...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>Gene</th>\n      <th>Variation</th>\n      <th>Class</th>\n      <th>Text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>FAM58A</td>\n      <td>Truncating Mutations</td>\n      <td>1</td>\n      <td>Cyclin-dependent kinases (CDKs) regulate a var...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>CBL</td>\n      <td>W802*</td>\n      <td>2</td>\n      <td>Abstract Background  Non-small cell lung canc...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>CBL</td>\n      <td>Q249E</td>\n      <td>2</td>\n      <td>Abstract Background  Non-small cell lung canc...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>CBL</td>\n      <td>N454D</td>\n      <td>3</td>\n      <td>Recent evidence has demonstrated that acquired...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>CBL</td>\n      <td>L399V</td>\n      <td>4</td>\n      <td>Oncogenic mutations in the monomeric Casitas B...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train.dropna(axis=0, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-12-18T09:39:09.629509Z","iopub.execute_input":"2023-12-18T09:39:09.629815Z","iopub.status.idle":"2023-12-18T09:39:09.641237Z","shell.execute_reply.started":"2023-12-18T09:39:09.629789Z","shell.execute_reply":"2023-12-18T09:39:09.640289Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"fig = px.pie(\n    values=train['Class'].value_counts(normalize=True).values,\n    names=train['Class'].value_counts(normalize=True).index,\n    title='Class Distribution'\n)\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-18T09:39:09.642442Z","iopub.execute_input":"2023-12-18T09:39:09.642767Z","iopub.status.idle":"2023-12-18T09:39:11.090997Z","shell.execute_reply.started":"2023-12-18T09:39:09.642738Z","shell.execute_reply":"2023-12-18T09:39:11.090175Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/html":"        <script type=\"text/javascript\">\n        window.PlotlyConfig = {MathJaxConfig: 'local'};\n        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n        if (typeof require !== 'undefined') {\n        require.undef(\"plotly\");\n        requirejs.config({\n            paths: {\n                'plotly': ['https://cdn.plot.ly/plotly-2.20.0.min']\n            }\n        });\n        require(['plotly'], function(Plotly) {\n            window._Plotly = Plotly;\n        });\n        }\n        </script>\n        "},"metadata":{}},{"output_type":"display_data","data":{"text/html":"<div>                            <div id=\"1e8485bb-b056-46a7-8fae-2d0b952fc5c5\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"1e8485bb-b056-46a7-8fae-2d0b952fc5c5\")) {                    Plotly.newPlot(                        \"1e8485bb-b056-46a7-8fae-2d0b952fc5c5\",                        [{\"domain\":{\"x\":[0.0,1.0],\"y\":[0.0,1.0]},\"hovertemplate\":\"label=%{label}<br>value=%{value}<extra></extra>\",\"labels\":[7,4,1,2,6,5,3,9,8],\"legendgroup\":\"\",\"name\":\"\",\"showlegend\":true,\"values\":[0.2870928829915561,0.206875753920386,0.1706875753920386,0.13630880579010857,0.08232810615199035,0.07297949336550061,0.02683956574185766,0.011158021712907118,0.005729794933655006],\"type\":\"pie\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"Class Distribution\"}},                        {\"responsive\": true}                    ).then(function(){\n                            \nvar gd = document.getElementById('1e8485bb-b056-46a7-8fae-2d0b952fc5c5');\nvar x = new MutationObserver(function (mutations, observer) {{\n        var display = window.getComputedStyle(gd).display;\n        if (!display || display === 'none') {{\n            console.log([gd, 'removed!']);\n            Plotly.purge(gd);\n            observer.disconnect();\n        }}\n}});\n\n// Listen for the removal of the full notebook cells\nvar notebookContainer = gd.closest('#notebook-container');\nif (notebookContainer) {{\n    x.observe(notebookContainer, {childList: true});\n}}\n\n// Listen for the clearing of the current output cell\nvar outputEl = gd.closest('.output');\nif (outputEl) {{\n    x.observe(outputEl, {childList: true});\n}}\n\n                        })                };                });            </script>        </div>"},"metadata":{}}]},{"cell_type":"code","source":"from datasets import Dataset,DatasetDict\ntrain_ds = Dataset.from_pandas(train)","metadata":{"execution":{"iopub.status.busy":"2023-12-18T09:39:11.092093Z","iopub.execute_input":"2023-12-18T09:39:11.092359Z","iopub.status.idle":"2023-12-18T09:39:12.416896Z","shell.execute_reply.started":"2023-12-18T09:39:11.092336Z","shell.execute_reply":"2023-12-18T09:39:12.415806Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"train['text len']= train['Text'].apply(lambda x: len(x.split()))\ntrain['text len'].max()","metadata":{"execution":{"iopub.status.busy":"2023-12-18T09:39:12.418129Z","iopub.execute_input":"2023-12-18T09:39:12.418757Z","iopub.status.idle":"2023-12-18T09:39:14.471205Z","shell.execute_reply.started":"2023-12-18T09:39:12.418728Z","shell.execute_reply":"2023-12-18T09:39:14.470221Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"76782"},"metadata":{}}]},{"cell_type":"markdown","source":"Tokenization","metadata":{}},{"cell_type":"code","source":"# for long text\nfrom transformers import LongformerTokenizer, LongformerForSequenceClassification\nmodel_name = \"allenai/longformer-base-4096\"\nmodel = LongformerForSequenceClassification.from_pretrained(model_name, num_labels=9)\ntokenizer = LongformerTokenizer.from_pretrained(model_name)\n\ndef tokenize_function(examples):\n    # Tokenize the text and return the tokenized inputs as a dictionary\n    inputs = tokenizer(examples['Text'], padding=True, truncation=True, max_length=100000)\n    return inputs\n\ntrain_tok = train_ds.map(tokenize_function, batched=True)\ntrain_tok = train_tok.rename_columns({'Text':'labels'})\n# Assuming tok_dataset is a tokenized dataset\nsample = train_tok[0]\n\n# Check if 'labels' key is present and if its value is a flat list of integers\nif 'labels' in sample and isinstance(sample['labels'], list) and all(isinstance(label, int) for label in sample['labels']):\n    print(\"Labels are flattened.\")\nelse:\n    print(\"Labels are not flattened.\")","metadata":{"execution":{"iopub.status.busy":"2023-12-18T11:32:27.020879Z","iopub.execute_input":"2023-12-18T11:32:27.021307Z","iopub.status.idle":"2023-12-18T11:41:04.101631Z","shell.execute_reply.started":"2023-12-18T11:32:27.021272Z","shell.execute_reply":"2023-12-18T11:41:04.100633Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90e970fe1d38485db43a924040ff6005"}},"metadata":{}},{"name":"stdout","text":"Labels are not flattened.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Embedding","metadata":{}},{"cell_type":"code","source":"#pip install gensim","metadata":{"execution":{"iopub.status.busy":"2023-12-18T11:29:18.736265Z","iopub.execute_input":"2023-12-18T11:29:18.737139Z","iopub.status.idle":"2023-12-18T11:29:30.792313Z","shell.execute_reply.started":"2023-12-18T11:29:18.737094Z","shell.execute_reply":"2023-12-18T11:29:30.791117Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Requirement already satisfied: gensim in /opt/conda/lib/python3.10/site-packages (4.3.1)\nRequirement already satisfied: numpy>=1.18.5 in /opt/conda/lib/python3.10/site-packages (from gensim) (1.23.5)\nRequirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.10/site-packages (from gensim) (6.3.0)\nRequirement already satisfied: scipy>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from gensim) (1.9.3)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"from gensim.models import KeyedVectors\nimport numpy as np\n\n# Load pre-trained GloVe embeddings\nglove_model = KeyedVectors.load_word2vec_format('path/to/glove.6B.50d.txt', binary=False)\n\n# Assuming train_tok has a 'tokens' column containing tokenized text\ntokens_column = train_tok['labels']\n\n# Define the dimension of your GloVe embeddings (50 in this case)\nembedding_dim = 50\n\n# Create an array to store the embeddings\nembeddings_matrix = np.zeros((len(tokens_column), len(tokens_column[0]), embedding_dim))\n\n# Iterate through each token in each row and assign the GloVe embedding\nfor i, tokens in enumerate(tokens_column):\n    for j, token in enumerate(tokens):\n        try:\n            embeddings_matrix[i, j, :] = glove_model[token]\n        except KeyError:\n            # Handle out-of-vocabulary tokens (initialize randomly or with zeros)\n            embeddings_matrix[i, j, :] = np.random.normal(size=(embedding_dim,))","metadata":{"execution":{"iopub.status.busy":"2023-12-18T11:51:54.371984Z","iopub.execute_input":"2023-12-18T11:51:54.372693Z","iopub.status.idle":"2023-12-18T11:51:54.674157Z","shell.execute_reply.started":"2023-12-18T11:51:54.372658Z","shell.execute_reply":"2023-12-18T11:51:54.672810Z"},"trusted":true},"execution_count":20,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[20], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Load pre-trained GloVe embeddings\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m glove_model \u001b[38;5;241m=\u001b[39m \u001b[43mKeyedVectors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_word2vec_format\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpath/to/glove.6B.50d.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Assuming train_tok has a 'tokens' column containing tokenized text\u001b[39;00m\n\u001b[1;32m      8\u001b[0m tokens_column \u001b[38;5;241m=\u001b[39m train_tok[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/gensim/models/keyedvectors.py:1719\u001b[0m, in \u001b[0;36mKeyedVectors.load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[1;32m   1672\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   1673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_word2vec_format\u001b[39m(\n\u001b[1;32m   1674\u001b[0m         \u001b[38;5;28mcls\u001b[39m, fname, fvocab\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m'\u001b[39m, unicode_errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1675\u001b[0m         limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, datatype\u001b[38;5;241m=\u001b[39mREAL, no_header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1676\u001b[0m     ):\n\u001b[1;32m   1677\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load KeyedVectors from a file produced by the original C word2vec-tool format.\u001b[39;00m\n\u001b[1;32m   1678\u001b[0m \n\u001b[1;32m   1679\u001b[0m \u001b[38;5;124;03m    Warnings\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1717\u001b[0m \n\u001b[1;32m   1718\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1719\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load_word2vec_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1720\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbinary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43municode_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43municode_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1721\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatatype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatatype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mno_header\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mno_header\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1722\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/gensim/models/keyedvectors.py:2048\u001b[0m, in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[1;32m   2045\u001b[0m             counts[word] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(count)\n\u001b[1;32m   2047\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading projection weights from \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, fname)\n\u001b[0;32m-> 2048\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fin:\n\u001b[1;32m   2049\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m no_header:\n\u001b[1;32m   2050\u001b[0m         \u001b[38;5;66;03m# deduce both vocab_size & vector_size from 1st pass over file\u001b[39;00m\n\u001b[1;32m   2051\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m binary:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/smart_open/smart_open_lib.py:177\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, compression, transport_params)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transport_params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    175\u001b[0m     transport_params \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 177\u001b[0m fobj \u001b[38;5;241m=\u001b[39m \u001b[43m_shortcut_open\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbuffering\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffering\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnewline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fobj\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/smart_open/smart_open_lib.py:363\u001b[0m, in \u001b[0;36m_shortcut_open\u001b[0;34m(uri, mode, compression, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m    361\u001b[0m     open_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124merrors\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m errors\n\u001b[0;32m--> 363\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_builtin_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocal_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffering\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffering\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mopen_kwargs\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'path/to/glove.6B.50d.txt'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'path/to/glove.6B.50d.txt'","output_type":"error"}]},{"cell_type":"markdown","source":"logistic regression","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report\nimport numpy as np\n\n# Flatten the embeddings_matrix\nflattened_embeddings = embeddings_matrix.reshape((len(tokens_column), -1))\n\n# Assuming train_tok has a 'classification_labels' column containing classification targets\nclassification_labels = train_tok['Class']\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(flattened_embeddings, classification_labels, test_size=0.2, random_state=42)\n\n# Initialize and train a classification model (e.g., Logistic Regression)\nclassification_model = LogisticRegression()\nclassification_model.fit(X_train, y_train)\n\n# Make predictions on the test set\npredictions = classification_model.predict(X_test)\n\n# Evaluate the classification model\naccuracy = accuracy_score(y_test, predictions)\nreport = classification_report(y_test, predictions)\n\nprint(f\"Accuracy: {accuracy}\")\nprint(\"Classification Report:\")\nprint(report)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndds = train_tok.train_test_split(0.25, seed=42)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"support vector machines","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, classification_report\nimport numpy as np\n\n# Flatten the embeddings_matrix\nflattened_embeddings = embeddings_matrix.reshape((len(tokens_column), -1))\n\n# Assuming train_tok has a 'classification_labels' column containing classification targets\nclassification_labels = train_tok['Class']\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(flattened_embeddings, classification_labels, test_size=0.2, random_state=42)\n\n# Initialize and train an SVM classifier\nsvm_classifier = SVC(kernel='linear')  # You can choose different kernels (linear, rbf, poly, etc.)\nsvm_classifier.fit(X_train, y_train)\n\n# Make predictions on the test set\npredictions = svm_classifier.predict(X_test)\n\n# Evaluate the SVM classifier\naccuracy = accuracy_score(y_test, predictions)\nreport = classification_report(y_test, predictions)\n\nprint(f\"Accuracy: {accuracy}\")\nprint(\"Classification Report:\")\nprint(report)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"random forests","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\nimport numpy as np\n\n# Flatten the embeddings_matrix\nflattened_embeddings = embeddings_matrix.reshape((len(tokens_column), -1))\n\n# Assuming train_tok has a 'classification_labels' column containing classification targets\nclassification_labels = train_tok['Class']\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(flattened_embeddings, classification_labels, test_size=0.2, random_state=42)\n\n# Initialize and train a Random Forest classifier\nrf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)  # You can adjust the number of trees (n_estimators)\nrf_classifier.fit(X_train, y_train)\n\n# Make predictions on the test set\npredictions = rf_classifier.predict(X_test)\n\n# Evaluate the Random Forest classifier\naccuracy = accuracy_score(y_test, predictions)\nreport = classification_report(y_test, predictions)\n\nprint(f\"Accuracy: {accuracy}\")\nprint(\"Classification Report:\")\nprint(report)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"transformers (e.g., BERT, GPT)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"3\"></a>\n<h2 style='background:lightblue; border:0; color:white'><center>3. Arguments</center><h2>","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification,AutoTokenizer\nfrom transformers import TrainingArguments,Trainer\n","metadata":{"execution":{"iopub.status.busy":"2023-12-18T09:39:14.472312Z","iopub.execute_input":"2023-12-18T09:39:14.472583Z","iopub.status.idle":"2023-12-18T09:39:18.553405Z","shell.execute_reply.started":"2023-12-18T09:39:14.472560Z","shell.execute_reply":"2023-12-18T09:39:18.552580Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"batch_size = 32\nepochs = 4\nlr = 8e-5\n\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\n\ndef compute_metrics(pred):\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(-1)\n    \n    accuracy = accuracy_score(labels, preds)\n    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n    \n    return {\n        'accuracy': accuracy,\n        'precision': precision,\n        'recall': recall,\n        'f1': f1,\n    }","metadata":{"execution":{"iopub.status.busy":"2023-12-18T09:39:18.554747Z","iopub.execute_input":"2023-12-18T09:39:18.555049Z","iopub.status.idle":"2023-12-18T09:39:18.562870Z","shell.execute_reply.started":"2023-12-18T09:39:18.555007Z","shell.execute_reply":"2023-12-18T09:39:18.562019Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"#fine-tune\nargs = TrainingArguments(\n    output_dir='./output',\n    logging_dir='./logs',\n    \n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size*2,\n    \n    num_train_epochs=epochs,\n    evaluation_strategy=\"epoch\", #Specifies after what to evaluate the model during training:\"epoch\", \"steps\",\"no\" \n    \n    learning_rate=lr,\n    lr_scheduler_type='cosine', #a cosine annealing scheduler: The learning rate starts high and gradually decreases in a cosine pattern, \n                                #helping the model converge more smoothly.\n                                #'linear', 'constant', or custom learning rate schedulers.\n    warmup_ratio=0.1, #Warmup is a technique where the learning rate starts very low and gradually increases for a certain number of steps. \n                      #It helps stabilize training at the beginning.\n                      #The warmup ratio is the fraction of total training steps during which the warmup is applied.\n    weight_decay=0.01, # regularization technique that penalizes large weights in the model.\n                       #This parameter specifies the strength of the weight decay. \n                       #A higher value means stronger regularization.\n    \n    logging_steps=500,#Log training information every logging_steps steps\n    do_train=True,\n    fp16=False,  #Whether to use mixed-precision training (16-bit floating-point) to reduce memory usage.\n    report_to='none'\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-18T09:39:18.564243Z","iopub.execute_input":"2023-12-18T09:39:18.564537Z","iopub.status.idle":"2023-12-18T09:39:18.660309Z","shell.execute_reply.started":"2023-12-18T09:39:18.564513Z","shell.execute_reply":"2023-12-18T09:39:18.659270Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"\nBefore tokenization, there are several important considerations to keep in mind:\n\n**Tokenization Method:**\n\nChoose an appropriate tokenizer based on your task and the model you intend to use. Different models may have different tokenization requirements.\n\n**Special Tokens:**\n\nBe aware of any special tokens that your model requires, such as [CLS] and [SEP] tokens for many sequence classification tasks.\n\n**Maximum Sequence Length:**\n\nCheck the maximum sequence length supported by your model. If your input sequences are longer than the model's maximum, you may need to truncate or split them.\n\n**Padding and Truncation:**\n\nDecide on a strategy for handling sequences that are shorter or longer than the desired length. Padding is often used for shorter sequences, and truncation may be necessary for longer sequences.\n\n**Attention Masks:**\n\nUnderstand the concept of attention masks. They are used to indicate which tokens are actual input and which are padding tokens. Ensure that your tokenization process generates attention masks correctly.\n\n**Special Tokens for Labels:**\n\nIn some tasks, you might need to include special tokens to represent labels or targets. Make sure you understand the specific requirements for your task.\n\n**Token Indexing:**\n\nVerify how tokens are indexed. Some tokenizers return token IDs directly, while others return tokens that need to be converted to IDs.\n\n**Flattening Nested Labels:**\n\nIf your labels have nested structures, like a list of lists, you may need to flatten them to a single list of integers before tokenization.\n\n**Vocabulary Size:**\n\nBe aware of the vocabulary size of your tokenizer. If your text contains out-of-vocabulary words, you may need to handle them appropriately.\n\n**Encoding Formats:**\n\nUnderstand the encoding formats your model accepts (e.g., PyTorch tensors, TensorFlow tensors). Ensure that your tokenization process outputs the appropriate format.","metadata":{}},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-12-18T10:47:43.445427Z","iopub.execute_input":"2023-12-18T10:47:43.445797Z","iopub.status.idle":"2023-12-18T10:56:24.920932Z","shell.execute_reply.started":"2023-12-18T10:47:43.445769Z","shell.execute_reply":"2023-12-18T10:56:24.920076Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e0e8a2e5e3042c68add6bbacbf8d1c6"}},"metadata":{}}]},{"cell_type":"code","source":"train_tok = train_tok.rename_columns({'Text':'labels'})\ndds = train_tok.train_test_split(0.25, seed=42)","metadata":{"execution":{"iopub.status.busy":"2023-12-18T11:12:15.796161Z","iopub.execute_input":"2023-12-18T11:12:15.796536Z","iopub.status.idle":"2023-12-18T11:12:15.812817Z","shell.execute_reply.started":"2023-12-18T11:12:15.796511Z","shell.execute_reply":"2023-12-18T11:12:15.811792Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"\n","metadata":{"execution":{"iopub.status.busy":"2023-12-18T11:12:28.386658Z","iopub.execute_input":"2023-12-18T11:12:28.387493Z","iopub.status.idle":"2023-12-18T11:12:28.523455Z","shell.execute_reply.started":"2023-12-18T11:12:28.387458Z","shell.execute_reply":"2023-12-18T11:12:28.522496Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Labels are not flattened.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Training\ntrainer = Trainer(\n    model,\n    args,\n    train_dataset=dds['train'],\n    eval_dataset=dds['test'],\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics\n)\n\ntrainer.train()\n\n# Save the fine-tuned model\nmodel.save_pretrained('./fine_tuned_model')\ntokenizer.save_pretrained('./fine_tuned_model')","metadata":{"execution":{"iopub.status.busy":"2023-12-18T11:13:14.961749Z","iopub.execute_input":"2023-12-18T11:13:14.962496Z","iopub.status.idle":"2023-12-18T11:13:29.849616Z","shell.execute_reply.started":"2023-12-18T11:13:14.962458Z","shell.execute_reply":"2023-12-18T11:13:29.848100Z"},"trusted":true},"execution_count":17,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:717\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor(value):\n\u001b[0;32m--> 717\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m \u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;66;03m# Removing this for now in favor of controlling the shape with `prepend_batch_axis`\u001b[39;00m\n\u001b[1;32m    720\u001b[0m     \u001b[38;5;66;03m# # at-least2d\u001b[39;00m\n\u001b[1;32m    721\u001b[0m     \u001b[38;5;66;03m# if tensor.ndim > 2:\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     \u001b[38;5;66;03m#     tensor = tensor.squeeze(0)\u001b[39;00m\n\u001b[1;32m    723\u001b[0m     \u001b[38;5;66;03m# elif tensor.ndim < 2:\u001b[39;00m\n\u001b[1;32m    724\u001b[0m     \u001b[38;5;66;03m#     tensor = tensor[None, :]\u001b[39;00m\n","\u001b[0;31mValueError\u001b[0m: too many dimensions 'str'","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[17], line 11\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[1;32m      2\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m      3\u001b[0m     model,\n\u001b[1;32m      4\u001b[0m     args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics\n\u001b[1;32m      9\u001b[0m )\n\u001b[0;32m---> 11\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Save the fine-tuned model\u001b[39;00m\n\u001b[1;32m     14\u001b[0m model\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./fine_tuned_model\u001b[39m\u001b[38;5;124m'\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1662\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1657\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1659\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1661\u001b[0m )\n\u001b[0;32m-> 1662\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1899\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1896\u001b[0m     rng_to_sync \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1898\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1899\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(epoch_iterator):\n\u001b[1;32m   1900\u001b[0m     total_batched_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1901\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rng_to_sync:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/data/data_collator.py:249\u001b[0m, in \u001b[0;36mDataCollatorWithPadding.__call__\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, features: List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[0;32m--> 249\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[1;32m    257\u001b[0m         batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3035\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.pad\u001b[0;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[1;32m   3032\u001b[0m             batch_outputs[key] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   3033\u001b[0m         batch_outputs[key]\u001b[38;5;241m.\u001b[39mappend(value)\n\u001b[0;32m-> 3035\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBatchEncoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:210\u001b[0m, in \u001b[0;36mBatchEncoding.__init__\u001b[0;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[1;32m    206\u001b[0m     n_sequences \u001b[38;5;241m=\u001b[39m encoding[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mn_sequences\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_sequences \u001b[38;5;241m=\u001b[39m n_sequences\n\u001b[0;32m--> 210\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:733\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    728\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverflowing_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    729\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    730\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to create tensor returning overflowing tokens of different lengths. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    731\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease see if a fast version of this tokenizer is available to have this feature available.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    732\u001b[0m             ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m--> 733\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    734\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to create tensor, you should probably activate truncation and/or padding with\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    735\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpadding=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtruncation=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to have batched tensors with the same length. Perhaps your\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    736\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m features (`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` in this case) have excessive nesting (inputs type `list` where type `int` is\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    737\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m expected).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    738\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    740\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n","\u001b[0;31mValueError\u001b[0m: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`labels` in this case) have excessive nesting (inputs type `list` where type `int` is expected)."],"ename":"ValueError","evalue":"Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`labels` in this case) have excessive nesting (inputs type `list` where type `int` is expected).","output_type":"error"}]},{"cell_type":"markdown","source":"<a id=\"3\"></a>\n<h2 style='background:lightblue; border:0; color:white'><center>3. Hugging Face and Bigscience</center><h2>","metadata":{}},{"cell_type":"code","source":"from transformers import T5Tokenizer, AutoModelForSeq2SeqLM\n\n# Update tokenizer\ntokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\ndef tokenize_function(examples):\n    # Tokenize the text\n    inputs = tokenizer(examples['Text'], return_tensors='pt', padding=True, truncation=True)\n    \n    # Ensure 'labels' is converted to a list\n    labels = examples['Text']\n    if isinstance(labels, torch.Tensor):\n        labels = labels.tolist()\n    \n    # Return a dictionary with inputs and labels as lists\n    return {\n        'input_ids': inputs['input_ids'].tolist(),\n        'attention_mask': inputs['attention_mask'].tolist(),\n        'labels': labels,\n    }\ntok_train = train_ds.map(tokenize_function, batched=True)\ntok_train = tok_train.rename_columns({'Text':'labels'})\ndds = tok_train.train_test_split(0.25, seed=42)\n\n# Fine-tune model configuration and training arguments\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"bigscience/T0_3B\")\n\n# Training\ntrainer = Trainer(\n    model,\n    args,\n    train_dataset=dds['train'],\n    eval_dataset=dds['test'],\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics\n)\n\ntrainer.train()\n\n# Save the fine-tuned model\nmodel.save_pretrained('./fine_tuned_model')\ntokenizer.save_pretrained('./fine_tuned_model')\n","metadata":{"execution":{"iopub.status.busy":"2023-12-18T09:42:36.728786Z","iopub.status.idle":"2023-12-18T09:42:36.729274Z","shell.execute_reply.started":"2023-12-18T09:42:36.729027Z","shell.execute_reply":"2023-12-18T09:42:36.729069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4\"></a>\n<h2 style='background:lightblue; border:0; color:white'><center>4. DeBERTa-v2 model architecture</center><h2>","metadata":{}},{"cell_type":"code","source":"from transformers import DebertaV2TokenizerFast, DebertaV2ForSequenceClassification\n# Load pre-trained model and tokenizer\nmodel_nm = 'microsoft/deberta-v2-xlarge'\nmodel = DebertaV2ForSequenceClassification.from_pretrained(model_nm, num_labels=9)\ntokenizer = DebertaV2TokenizerFast.from_pretrained(model_nm)\n\n#labels feature has excessive nesting, \n#you need to ensure that it is flattened to a list of integers before tokenization\ndef tokenize_function(examples):\n    # Flatten the nested list of labels to a flat list of integers\n    flat_labels = [label for sublist in examples['Text'] for label in sublist]\n    # Tokenize the text and return the tokenized inputs\n    return tokenizer(examples['Text'], labels=flat_labels, padding=True, truncation=True, return_tensors='pt')\n\n\ntok_train = train_ds.map(tokenize_function, batched=True)\ntok_train = tok_train.rename_columns({'Text':'labels'})\ndds = tok_train.train_test_split(0.25, seed=42)\ndds","metadata":{"execution":{"iopub.status.busy":"2023-12-18T09:42:36.730608Z","iopub.status.idle":"2023-12-18T09:42:36.730922Z","shell.execute_reply.started":"2023-12-18T09:42:36.730768Z","shell.execute_reply":"2023-12-18T09:42:36.730783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer = Trainer(model, \n                  args, \n                  train_dataset=dds['train'], \n                  eval_dataset=dds['test'],\n                  tokenizer=tokenizer, \n                  compute_metrics=compute_metrics)\n\ntrainer.train()\n\n# Save the fine-tuned model\nmodel.save_pretrained('./fine_tuned_model')\ntokenizer.save_pretrained('./fine_tuned_model')","metadata":{"execution":{"iopub.status.busy":"2023-12-18T09:42:36.732413Z","iopub.status.idle":"2023-12-18T09:42:36.732749Z","shell.execute_reply.started":"2023-12-18T09:42:36.732587Z","shell.execute_reply":"2023-12-18T09:42:36.732603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_text = pd.read_csv('/kaggle/input/unzip-files/test_text',\n                         sep='\\|\\|', \n                         header=None, \n                         skiprows=1, \n                         names=[\"ID\",\"Text\"])\ntest_var = pd.read_csv('/kaggle/input/unzip-files/test_variants')\ntest = test_var.merge(test_text, how='left', on='ID')\ntest.dropna(axis=0, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-12-18T09:42:36.733798Z","iopub.status.idle":"2023-12-18T09:42:36.734145Z","shell.execute_reply.started":"2023-12-18T09:42:36.733950Z","shell.execute_reply":"2023-12-18T09:42:36.733964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_ds = Dataset.from_pandas(test.map(tok_func, batched=True)\npreds = trainer.predict(test_true_ds).predictions.astype(int)\nfrom sklearn.metrics import classification_report\n\n# Assuming you have true labels in test_true_ds\ntrue_labels = test_true['Sentiment']\n\n# Convert predicted probabilities to labels\npred_labels = preds.argmax(axis=1)\n\n# Generate a classification report\nclass_report = classification_report(true_labels, pred_labels)\n\n# Print or use the classification report as needed\nprint(class_report)","metadata":{"execution":{"iopub.status.busy":"2023-12-18T09:42:36.735657Z","iopub.status.idle":"2023-12-18T09:42:36.735988Z","shell.execute_reply.started":"2023-12-18T09:42:36.735828Z","shell.execute_reply":"2023-12-18T09:42:36.735844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_true = pd.read_csv('/kaggle/input/covid-19-nlp-text-classification/Corona_NLP_test.csv', encoding='ISO-8859-1')\ntest_true['Sentiment']=coder.fit_transform(test_true['Sentiment'])\ndisplay(test_true.info(),\n       test_true.describe(),\n       test_true.head())\n\ntest_true_ds = Dataset.from_pandas(test_true).map(tok_func, batched=True)\npreds = trainer.predict(test_true_ds).predictions.astype(int)\nfrom sklearn.metrics import classification_report\n\n# Assuming you have true labels in test_true_ds\ntrue_labels = test_true['Sentiment']\n\n# Convert predicted probabilities to labels\npred_labels = preds.argmax(axis=1)\n\n# Generate a classification report\nclass_report = classification_report(true_labels, pred_labels)\n\n# Print or use the classification report as needed\nprint(class_report)","metadata":{"execution":{"iopub.status.busy":"2023-12-18T09:42:36.738856Z","iopub.status.idle":"2023-12-18T09:42:36.739361Z","shell.execute_reply.started":"2023-12-18T09:42:36.739112Z","shell.execute_reply":"2023-12-18T09:42:36.739135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test_txt = pd.read_csv('../input/test_text', sep='\\|\\|', header=None, skiprows=1, names=[\"ID\",\"Text\"])\ndf_test_txt.head()","metadata":{"_cell_guid":"7bf0c669-6284-4974-a996-54ad0e6941f6","_execution_state":"idle","_uuid":"65bb8b567ee893c6f796f2508b576e9c02e27a46","execution":{"iopub.status.busy":"2023-12-18T09:42:36.740846Z","iopub.status.idle":"2023-12-18T09:42:36.741361Z","shell.execute_reply.started":"2023-12-18T09:42:36.741103Z","shell.execute_reply":"2023-12-18T09:42:36.741127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test_var = pd.read_csv('../input/test_variants')\ndf_test_var.head()","metadata":{"_cell_guid":"dfc2969d-1ef9-4e89-a236-70e17e1b1970","_execution_state":"idle","_uuid":"53f36fe20604e87d1f083f59448da9d6ef0e3863","execution":{"iopub.status.busy":"2023-12-18T09:42:36.743320Z","iopub.status.idle":"2023-12-18T09:42:36.743763Z","shell.execute_reply.started":"2023-12-18T09:42:36.743532Z","shell.execute_reply":"2023-12-18T09:42:36.743553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Let's join them together","metadata":{"_cell_guid":"5707f8ac-85bc-40fd-9a8b-5e7912d5c8d1","_uuid":"2e9d1f1157305e68bb06ae32a00f2bb4e31710c8"}},{"cell_type":"code","source":"df_train = pd.merge(df_train_var, df_train_txt, how='left', on='ID')\ndf_train.head()","metadata":{"_cell_guid":"13b2c874-ef56-45f5-9500-aae3c77fbfaa","_execution_state":"idle","_uuid":"315a8b16269f0a2894c4d82642e82baa7826b30b","execution":{"iopub.status.busy":"2023-12-18T09:42:36.745209Z","iopub.status.idle":"2023-12-18T09:42:36.745535Z","shell.execute_reply.started":"2023-12-18T09:42:36.745376Z","shell.execute_reply":"2023-12-18T09:42:36.745392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test = pd.merge(df_test_var, df_test_txt, how='left', on='ID')\ndf_test.head()","metadata":{"_cell_guid":"86a5146c-f1c2-4d41-8c5a-3fc620d16695","_execution_state":"idle","_uuid":"7c2b09211ba02dba759bdbc4b2feea5c0928582a","execution":{"iopub.status.busy":"2023-12-18T09:42:36.747553Z","iopub.status.idle":"2023-12-18T09:42:36.747921Z","shell.execute_reply.started":"2023-12-18T09:42:36.747729Z","shell.execute_reply":"2023-12-18T09:42:36.747745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Run preliminary statistics on loaded data","metadata":{"_cell_guid":"6902a7ac-d631-4ae3-b80b-b5fbacc7bf11","_uuid":"90c26968d60d11ac38dfdb2f3ffdd3b04acc4ba6"}},{"cell_type":"code","source":"df_train.describe(include='all')","metadata":{"_cell_guid":"2045e414-408a-46d8-a351-1757bd9df5e9","_execution_state":"idle","_uuid":"cc9d844d7110e9534dc40e019d885c5bcc31e904","execution":{"iopub.status.busy":"2023-12-18T09:42:36.749383Z","iopub.status.idle":"2023-12-18T09:42:36.749713Z","shell.execute_reply.started":"2023-12-18T09:42:36.749555Z","shell.execute_reply":"2023-12-18T09:42:36.749571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.describe(include='all')","metadata":{"_cell_guid":"db691b33-6370-4ec4-8086-77ea6528b568","_execution_state":"idle","_uuid":"360c0686091dbf914980ba1c398c6d5f1ad44bcd","execution":{"iopub.status.busy":"2023-12-18T09:42:36.751288Z","iopub.status.idle":"2023-12-18T09:42:36.751763Z","shell.execute_reply.started":"2023-12-18T09:42:36.751525Z","shell.execute_reply":"2023-12-18T09:42:36.751547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['Class'].value_counts().plot(kind=\"bar\", rot=0)","metadata":{"_cell_guid":"199c8014-d06f-4f14-b8fa-bf4ec2e35d21","_execution_state":"idle","_uuid":"3e5fcd9f545b5a80b793905fe2e562668d751536","execution":{"iopub.status.busy":"2023-12-18T09:42:36.752945Z","iopub.status.idle":"2023-12-18T09:42:36.753434Z","shell.execute_reply.started":"2023-12-18T09:42:36.753196Z","shell.execute_reply":"2023-12-18T09:42:36.753218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Classes seem very imbalanced","metadata":{"_cell_guid":"7f3c61af-91fe-4da4-9e58-cb5c050f45a7","_uuid":"5d1cfc6d2e8aa7052f99bf256988df4d9c72e231"}},{"cell_type":"code","source":"# This cell reduces the training data for Kaggle limits. Remove this cell for real results.\ndf_train, _ = train_test_split(df_train, test_size=0.7, random_state=8, stratify=df_train['Class'])\ndf_train.shape","metadata":{"_cell_guid":"414fba7c-ecae-4a7e-acfe-d926df6aaec5","_uuid":"fcb73d697f9b4656f5a2c728acc67cc48dd5e930","execution":{"iopub.status.busy":"2023-12-18T09:42:36.755231Z","iopub.status.idle":"2023-12-18T09:42:36.755673Z","shell.execute_reply.started":"2023-12-18T09:42:36.755441Z","shell.execute_reply":"2023-12-18T09:42:36.755462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# The main task here is to predict the class of the mutation given the text in the literature. Our approach will then be to apply some common NLP techniques to transform the free text into features for an ML classifier and see which ones work best.\n\n### Define a helper function to evaluate the effectiveness of transformed free text. We'll use a simple logistic regression with 3-fold stratified cross-validation for fast evaluation.","metadata":{"_cell_guid":"f6abf33d-337d-4be8-8d50-34d2292a4c71","_uuid":"525e0ac244f14d7dbc0edfe5783c1439a3a10d3d"}},{"cell_type":"code","source":"def evaluate_features(X, y, clf=None):\n    \"\"\"General helper function for evaluating effectiveness of passed features in ML model\n    \n    Prints out Log loss, accuracy, and confusion matrix with 3-fold stratified cross-validation\n    \n    Args:\n        X (array-like): Features array. Shape (n_samples, n_features)\n        \n        y (array-like): Labels array. Shape (n_samples,)\n        \n        clf: Classifier to use. If None, default Log reg is use.\n    \"\"\"\n    if clf is None:\n        clf = LogisticRegression()\n    \n    probas = cross_val_predict(clf, X, y, cv=StratifiedKFold(random_state=8), \n                              n_jobs=-1, method='predict_proba', verbose=2)\n    pred_indices = np.argmax(probas, axis=1)\n    classes = np.unique(y)\n    preds = classes[pred_indices]\n    print('Log loss: {}'.format(log_loss(y, probas)))\n    print('Accuracy: {}'.format(accuracy_score(y, preds)))\n    skplt.plot_confusion_matrix(y, preds)\n","metadata":{"_cell_guid":"e687a44c-45f1-47c0-8c8f-3cced55cfba7","_execution_state":"idle","_uuid":"74e93f2f49e6399758ccda8950741f2dc3070e70","execution":{"iopub.status.busy":"2023-12-18T09:42:36.756733Z","iopub.status.idle":"2023-12-18T09:42:36.757184Z","shell.execute_reply.started":"2023-12-18T09:42:36.756941Z","shell.execute_reply":"2023-12-18T09:42:36.756962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's do a quick test of evaluate_features","metadata":{"_cell_guid":"f123aa3e-e4a2-4bbd-82a4-deb9f9416749","_uuid":"05e65e23552a5c11fedaa9e9e65ac9156a9297e0"}},{"cell_type":"code","source":"# Quick test of evaluate_features\nfrom sklearn.datasets import load_iris\nevaluate_features(*load_iris(True))","metadata":{"_cell_guid":"96a33bf5-96a2-4099-8e09-a61db0c052e1","_execution_state":"idle","_uuid":"1795526eb77ec9281a8a18f3f01d02cc4b643a9f","execution":{"iopub.status.busy":"2023-12-18T09:42:36.759175Z","iopub.status.idle":"2023-12-18T09:42:36.759500Z","shell.execute_reply.started":"2023-12-18T09:42:36.759347Z","shell.execute_reply":"2023-12-18T09:42:36.759362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Start with a simple baseline. Bag of words","metadata":{"_cell_guid":"98e9cd4a-f6f4-4c5b-a151-2edf4ac179f6","_uuid":"22078fdc406c13d21d1588644371edc43a553b1a"}},{"cell_type":"code","source":"count_vectorizer = CountVectorizer(\n    analyzer=\"word\", tokenizer=nltk.word_tokenize,\n    preprocessor=None, stop_words='english', max_features=None)    \n","metadata":{"_cell_guid":"d3207228-9664-472e-b025-7507f10fec7c","_execution_state":"idle","_uuid":"5028611d6b4007f8f139b432c7909748781c3022","execution":{"iopub.status.busy":"2023-12-18T09:42:36.760399Z","iopub.status.idle":"2023-12-18T09:42:36.760732Z","shell.execute_reply.started":"2023-12-18T09:42:36.760572Z","shell.execute_reply":"2023-12-18T09:42:36.760588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bag_of_words = count_vectorizer.fit_transform(df_train['Text'])","metadata":{"_cell_guid":"5df3c9c0-4b0b-487f-bef4-6cceb8ce0290","_execution_state":"busy","_uuid":"1153f8a8a0320483ce8c8f3044fe21cb860ad989","execution":{"iopub.status.busy":"2023-12-18T09:42:36.761801Z","iopub.status.idle":"2023-12-18T09:42:36.762132Z","shell.execute_reply.started":"2023-12-18T09:42:36.761948Z","shell.execute_reply":"2023-12-18T09:42:36.761962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(count_vectorizer.get_feature_names())","metadata":{"_cell_guid":"c037498b-d5d5-4c50-93a4-d37c5c8f091e","_execution_state":"busy","_uuid":"69e813164a0da79e98ec0895a4ce24759dbcdf79","execution":{"iopub.status.busy":"2023-12-18T09:42:36.763897Z","iopub.status.idle":"2023-12-18T09:42:36.764235Z","shell.execute_reply.started":"2023-12-18T09:42:36.764075Z","shell.execute_reply":"2023-12-18T09:42:36.764091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 281586 unique words in corpus","metadata":{"_cell_guid":"31960f4d-efd5-4162-bf78-5780088f4d5d","_uuid":"ea3d1136c7e969c8a03f6fb182885782396be325"}},{"cell_type":"code","source":"svd = TruncatedSVD(n_components=25, n_iter=25, random_state=12)\ntruncated_bag_of_words = svd.fit_transform(bag_of_words)","metadata":{"_cell_guid":"6a5ae966-e759-4c29-96de-7d4492e93888","_execution_state":"busy","_uuid":"74d91589176aed1f15f991fe7e2115df4229c107","execution":{"iopub.status.busy":"2023-12-18T09:42:36.765489Z","iopub.status.idle":"2023-12-18T09:42:36.765931Z","shell.execute_reply.started":"2023-12-18T09:42:36.765701Z","shell.execute_reply":"2023-12-18T09:42:36.765723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_features(truncated_bag_of_words, df_train['Class'].values.ravel())","metadata":{"_cell_guid":"15b83eea-8ea0-4b4b-a055-d109f7ce814a","_execution_state":"busy","_uuid":"9006201fdd9c50f26229fcbafa258b3dbef94b6b","execution":{"iopub.status.busy":"2023-12-18T09:42:36.767280Z","iopub.status.idle":"2023-12-18T09:42:36.767590Z","shell.execute_reply.started":"2023-12-18T09:42:36.767432Z","shell.execute_reply":"2023-12-18T09:42:36.767447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_features(truncated_bag_of_words, df_train['Class'].values.ravel(), \n                  RandomForestClassifier(n_estimators=1000, max_depth=5, verbose=1))","metadata":{"_cell_guid":"6abaa6b4-852b-4421-9f37-3bce113c357e","_execution_state":"busy","_uuid":"d78b3a8c686dbf57aadd3faf8ca1fa66e70bfa00","execution":{"iopub.status.busy":"2023-12-18T09:42:36.768762Z","iopub.status.idle":"2023-12-18T09:42:36.769140Z","shell.execute_reply.started":"2023-12-18T09:42:36.768934Z","shell.execute_reply":"2023-12-18T09:42:36.768950Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Bad results overall for the baseline\n\n## Let's try TFIDF","metadata":{"_cell_guid":"27ef74f3-6be8-4f87-8a43-4974a8a776ba","_uuid":"ea1c3dd8c62d52ed234d7ce20041e0df3334de46"}},{"cell_type":"code","source":"count_vectorizer = TfidfVectorizer(\n    analyzer=\"word\", tokenizer=nltk.word_tokenize,\n    preprocessor=None, stop_words='english', max_features=None)    \n\ntfidf = count_vectorizer.fit_transform(df_train['Text'])\n\nlen(count_vectorizer.get_feature_names())","metadata":{"_cell_guid":"80a12233-65b3-426c-8fab-8d15cc6c78ab","_execution_state":"busy","_uuid":"b9c41a68f6682de79329d161eaaa565063a13f08","execution":{"iopub.status.busy":"2023-12-18T09:42:36.770366Z","iopub.status.idle":"2023-12-18T09:42:36.770676Z","shell.execute_reply.started":"2023-12-18T09:42:36.770517Z","shell.execute_reply":"2023-12-18T09:42:36.770532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svd = TruncatedSVD(n_components=25, n_iter=25, random_state=12)\ntruncated_tfidf = svd.fit_transform(tfidf)","metadata":{"_cell_guid":"7befec83-dc74-48fb-97b0-c2d78b17decc","_execution_state":"busy","_uuid":"ef48809922c0850626803b3323585d60ff78a32f","execution":{"iopub.status.busy":"2023-12-18T09:42:36.772558Z","iopub.status.idle":"2023-12-18T09:42:36.772892Z","shell.execute_reply.started":"2023-12-18T09:42:36.772732Z","shell.execute_reply":"2023-12-18T09:42:36.772747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_features(truncated_tfidf, df_train['Class'].values.ravel())","metadata":{"_cell_guid":"91e06bc0-6ce2-4466-a8b1-66a555a94d3b","_execution_state":"busy","_uuid":"0bd566447e9f104b7b79bb56e8daff6cd372bebd","execution":{"iopub.status.busy":"2023-12-18T09:42:36.774277Z","iopub.status.idle":"2023-12-18T09:42:36.774621Z","shell.execute_reply.started":"2023-12-18T09:42:36.774451Z","shell.execute_reply":"2023-12-18T09:42:36.774466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_features(truncated_tfidf, df_train['Class'].values.ravel(), \n                  RandomForestClassifier(n_estimators=1000, max_depth=5, verbose=1))","metadata":{"_cell_guid":"775e52c8-d9e8-4db1-a46c-4f7416aa8a9a","_execution_state":"busy","_uuid":"87c8f656c58e997062d4c82df7f6b319bfa49874","execution":{"iopub.status.busy":"2023-12-18T09:42:36.775517Z","iopub.status.idle":"2023-12-18T09:42:36.775854Z","shell.execute_reply.started":"2023-12-18T09:42:36.775675Z","shell.execute_reply":"2023-12-18T09:42:36.775690Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_features(tfidf, df_train['Class'].values.ravel(), \n                  SVC(kernel='linear', probability=True))","metadata":{"_cell_guid":"8dc579fa-63ab-4919-ba78-087adcdb2043","_execution_state":"busy","_uuid":"dc4479f8be6d3f524ce0a086b8019035d50027b1","execution":{"iopub.status.busy":"2023-12-18T09:42:36.777059Z","iopub.status.idle":"2023-12-18T09:42:36.777385Z","shell.execute_reply.started":"2023-12-18T09:42:36.777224Z","shell.execute_reply":"2023-12-18T09:42:36.777240Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### A little better, but still bad. You can see from the confusion matrix that it's just classifying most samples into class 7.\n\n### Also tried a linear SVM for features straight from TFIDF (did not go through Truncated SVD). Worse log loss but confusion matrix seems to show better balance among predicted classes.\n\n_____\n\n## This time, let's try the popular word2vec to get features\n\n### Define helper function get_word2vec  and helper class MySentences for training word2vec on the corpus of texts. (or loading if already trained and saved to disk)","metadata":{"_cell_guid":"67253f68-410f-4309-bf17-2144340558c3","_uuid":"62cb5aa5763d6bf912fd56abc777998175806ca2"}},{"cell_type":"code","source":"class MySentences(object):\n    \"\"\"MySentences is a generator to produce a list of tokenized sentences \n    \n    Takes a list of numpy arrays containing documents.\n    \n    Args:\n        arrays: List of arrays, where each element in the array contains a document.\n    \"\"\"\n    def __init__(self, *arrays):\n        self.arrays = arrays\n \n    def __iter__(self):\n        for array in self.arrays:\n            for document in array:\n                for sent in nltk.sent_tokenize(document):\n                    yield nltk.word_tokenize(sent)\n\ndef get_word2vec(sentences, location):\n    \"\"\"Returns trained word2vec\n    \n    Args:\n        sentences: iterator for sentences\n        \n        location (str): Path to save/load word2vec\n    \"\"\"\n    if os.path.exists(location):\n        print('Found {}'.format(location))\n        model = gensim.models.Word2Vec.load(location)\n        return model\n    \n    print('{} not found. training model'.format(location))\n    model = gensim.models.Word2Vec(sentences, size=100, window=5, min_count=5, workers=4)\n    print('Model done training. Saving to disk')\n    model.save(location)\n    return model","metadata":{"_cell_guid":"609177df-1799-4f5e-b3e7-77c820561a7c","_execution_state":"busy","_uuid":"d631c05ece2e126a82481fa5a262d12ec7577e38","execution":{"iopub.status.busy":"2023-12-18T09:42:36.778389Z","iopub.status.idle":"2023-12-18T09:42:36.778735Z","shell.execute_reply.started":"2023-12-18T09:42:36.778566Z","shell.execute_reply":"2023-12-18T09:42:36.778583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Start training the word2vec model. Since word2vec training is unsupervised, you can use both training and test datasets.\n\nIf training has already been done, the function will just load the word2vec to disk so you don't need to retrain if rerunning the notebook","metadata":{"_cell_guid":"5782e63f-8fef-47c8-b83a-829836f1f096","_uuid":"70f109950197b133cc46e2497b2248830d42a349"}},{"cell_type":"code","source":"w2vec = get_word2vec(\n    MySentences(\n        df_train['Text'].values, \n        #df_test['Text'].values  Commented for Kaggle limits\n    ),\n    'w2vmodel'\n)","metadata":{"_cell_guid":"e62cf926-34d9-461a-bf92-21e38e9ff2d3","_execution_state":"busy","_uuid":"80d89a6184209c1630976aa5bdb9a93b1290b363","execution":{"iopub.status.busy":"2023-12-18T09:42:36.779784Z","iopub.status.idle":"2023-12-18T09:42:36.780132Z","shell.execute_reply.started":"2023-12-18T09:42:36.779946Z","shell.execute_reply":"2023-12-18T09:42:36.779961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Now that we have our word2vec model, how do we use it to transform each documents into a feature vector? In order to convert a document of multiple words into a single vector using our trained word2vec, we take the word2vec of all words in the document, then take its mean.\n\n### We'll define a transformer (with sklearn interface) to convert a document into its corresponding vector","metadata":{"_cell_guid":"591cc8cb-49c2-41d0-be0a-ff8ee7d4f230","_uuid":"b41f276a240b388d56fb5187a5b99ac0d3b76d59"}},{"cell_type":"code","source":"class MyTokenizer:\n    def __init__(self):\n        pass\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        transformed_X = []\n        for document in X:\n            tokenized_doc = []\n            for sent in nltk.sent_tokenize(document):\n                tokenized_doc += nltk.word_tokenize(sent)\n            transformed_X.append(np.array(tokenized_doc))\n        return np.array(transformed_X)\n    \n    def fit_transform(self, X, y=None):\n        return self.transform(X)\n\nclass MeanEmbeddingVectorizer(object):\n    def __init__(self, word2vec):\n        self.word2vec = word2vec\n        # if a text is empty we should return a vector of zeros\n        # with the same dimensionality as all the other vectors\n        self.dim = len(word2vec.wv.syn0[0])\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        X = MyTokenizer().fit_transform(X)\n        \n        return np.array([\n            np.mean([self.word2vec.wv[w] for w in words if w in self.word2vec.wv]\n                    or [np.zeros(self.dim)], axis=0)\n            for words in X\n        ])\n    \n    def fit_transform(self, X, y=None):\n        return self.transform(X)\n","metadata":{"_cell_guid":"8be99804-9357-4522-a80b-aa328f5ef973","_execution_state":"busy","_uuid":"f188944da320461ad2e8f76a7a991454e2c0763c","execution":{"iopub.status.busy":"2023-12-18T09:42:36.781427Z","iopub.status.idle":"2023-12-18T09:42:36.781759Z","shell.execute_reply.started":"2023-12-18T09:42:36.781597Z","shell.execute_reply":"2023-12-18T09:42:36.781612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mean_embedding_vectorizer = MeanEmbeddingVectorizer(w2vec)\nmean_embedded = mean_embedding_vectorizer.fit_transform(df_train['Text'])","metadata":{"_cell_guid":"1a13f2ef-1fb7-462a-8e72-63608f158969","_execution_state":"busy","_uuid":"22e86ded9c31b7039c65ba3e46a2e58ff2b08cee","execution":{"iopub.status.busy":"2023-12-18T09:42:36.784170Z","iopub.status.idle":"2023-12-18T09:42:36.785544Z","shell.execute_reply.started":"2023-12-18T09:42:36.785353Z","shell.execute_reply":"2023-12-18T09:42:36.785377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_features(mean_embedded, df_train['Class'].values.ravel())","metadata":{"_cell_guid":"383b2dc5-5ba8-4a44-86cc-d6710bacc8a8","_execution_state":"busy","_uuid":"2f30d63d72bc0dda91f1625a399fc1a98c8e25c8","execution":{"iopub.status.busy":"2023-12-18T09:42:36.786497Z","iopub.status.idle":"2023-12-18T09:42:36.786817Z","shell.execute_reply.started":"2023-12-18T09:42:36.786661Z","shell.execute_reply":"2023-12-18T09:42:36.786676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_features(mean_embedded, df_train['Class'].values.ravel(),\n                  RandomForestClassifier(n_estimators=1000, max_depth=15, verbose=1))","metadata":{"_cell_guid":"33e16fc2-7172-4163-9d6e-8bab7a9e85b3","_execution_state":"busy","_uuid":"c0fcd434367d7a1899deda82cdfe5726c85bb41e","execution":{"iopub.status.busy":"2023-12-18T09:42:36.788085Z","iopub.status.idle":"2023-12-18T09:42:36.788438Z","shell.execute_reply.started":"2023-12-18T09:42:36.788274Z","shell.execute_reply":"2023-12-18T09:42:36.788289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_features(mean_embedded, \n                  df_train['Class'].values.ravel(),\n                  XGBClassifier(max_depth=4,\n                                objective='multi:softprob',\n                                learning_rate=0.03333,\n                                )\n                 )","metadata":{"_cell_guid":"ffa542a9-909c-4bd6-a59f-de71f3327d9e","_execution_state":"busy","_uuid":"e18fab5b47d20ff4050b7b7d1c6e44f374772abb","execution":{"iopub.status.busy":"2023-12-18T09:42:36.789856Z","iopub.status.idle":"2023-12-18T09:42:36.790329Z","shell.execute_reply.started":"2023-12-18T09:42:36.790100Z","shell.execute_reply":"2023-12-18T09:42:36.790121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### As expected, we get better results than TF-IDF. \n\nThe results are still not very good though. One way to explain this is that there is a lot of information loss from just getting the mean of all word vectors of the document. This is roughly analogous to taking the entire document, summarizing it into one word, and using that word to classify the entire text.","metadata":{"_cell_guid":"d59000aa-abc6-4ea8-b3f8-683f54e95761","_uuid":"6ea65a9b884a0af3c31a2ca61432fa6858f9a402"}},{"cell_type":"markdown","source":"## Let's try a quick and dirty LSTM in Keras to take into account the sequential nature of text\n\n* We won't do any hyperparameter search \n* We'll go with 15 epochs, and save the model with the best validation loss after an epoch\n* Max sequence length is cut down to a measly 2000 (longest text has 77000+ words), to shorten training time and prevent GPU OOM\n\nNote: This takes about an hour to run on GPU","metadata":{"_cell_guid":"7db18b6e-d23f-412a-9f16-429082572649","_uuid":"934ddee5cd73ca17ee2bd73f647290697aca997c"}},{"cell_type":"code","source":"# Use the Keras tokenizer\nnum_words = 2000\ntokenizer = Tokenizer(num_words=num_words)\ntokenizer.fit_on_texts(df_train['Text'].values)","metadata":{"_cell_guid":"d1b0a192-ea18-419e-9ba4-bb52f12b9f78","_execution_state":"busy","_uuid":"8d4ee34e1e95faa8be9d93ecced116c324b61465","execution":{"iopub.status.busy":"2023-12-18T09:42:36.791351Z","iopub.status.idle":"2023-12-18T09:42:36.791790Z","shell.execute_reply.started":"2023-12-18T09:42:36.791567Z","shell.execute_reply":"2023-12-18T09:42:36.791588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Pad the data \nX = tokenizer.texts_to_sequences(df_train['Text'].values)\nX = pad_sequences(X, maxlen=2000)","metadata":{"_cell_guid":"0610df13-711b-4d28-810a-2f7361cea388","_execution_state":"busy","_uuid":"2fba830b1fd73426d1b3b2f22bac503d91d0a923","execution":{"iopub.status.busy":"2023-12-18T09:42:36.792971Z","iopub.status.idle":"2023-12-18T09:42:36.793419Z","shell.execute_reply.started":"2023-12-18T09:42:36.793197Z","shell.execute_reply":"2023-12-18T09:42:36.793219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Build out our simple LSTM\nembed_dim = 128\nlstm_out = 196\n\n# Model saving callback\nckpt_callback = ModelCheckpoint('keras_model', \n                                 monitor='val_loss', \n                                 verbose=1, \n                                 save_best_only=True, \n                                 mode='auto')\n\nmodel = Sequential()\nmodel.add(Embedding(num_words, embed_dim, input_length = X.shape[1]))\nmodel.add(LSTM(lstm_out, recurrent_dropout=0.2, dropout=0.2))\nmodel.add(Dense(9,activation='softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics = ['categorical_crossentropy'])\nprint(model.summary())","metadata":{"_cell_guid":"af64e666-69b4-4fe7-a8ce-c098e640cbf9","_execution_state":"busy","_uuid":"6aff6b56a17d5bc0650e1bb41e60fee7be6f52ac","execution":{"iopub.status.busy":"2023-12-18T09:42:36.794835Z","iopub.status.idle":"2023-12-18T09:42:36.795181Z","shell.execute_reply.started":"2023-12-18T09:42:36.794989Z","shell.execute_reply":"2023-12-18T09:42:36.795004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y = pd.get_dummies(df_train['Class']).values\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 42, stratify=Y)\nprint(X_train.shape, Y_train.shape)\nprint(X_test.shape, Y_test.shape)\n","metadata":{"_cell_guid":"3a3d8330-3b5e-4cb6-87c7-0f7707b48a76","_execution_state":"busy","_uuid":"f9a53d657ec8a6017648965ad8073627756a13bd","execution":{"iopub.status.busy":"2023-12-18T09:42:36.796692Z","iopub.status.idle":"2023-12-18T09:42:36.797062Z","shell.execute_reply.started":"2023-12-18T09:42:36.796867Z","shell.execute_reply":"2023-12-18T09:42:36.796884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 32\nmodel.fit(X_train, Y_train, epochs=8, batch_size=batch_size, validation_split=0.2, callbacks=[ckpt_callback])","metadata":{"scrolled":true,"_cell_guid":"d7eac40d-a8d6-47a7-9542-1a2b06f76ca8","_execution_state":"busy","_uuid":"a0da0f4a43684380e3dd1b6f53d03cc45384fbf9","execution":{"iopub.status.busy":"2023-12-18T09:42:36.798245Z","iopub.status.idle":"2023-12-18T09:42:36.798577Z","shell.execute_reply.started":"2023-12-18T09:42:36.798408Z","shell.execute_reply":"2023-12-18T09:42:36.798423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = load_model('keras_model')","metadata":{"_cell_guid":"72101f81-dd5a-4b27-b985-e4fa03a7d7fd","_execution_state":"busy","_uuid":"142200cc3855cd276bb3a9abf3526a61988fafee","execution":{"iopub.status.busy":"2023-12-18T09:42:36.799754Z","iopub.status.idle":"2023-12-18T09:42:36.800136Z","shell.execute_reply.started":"2023-12-18T09:42:36.799929Z","shell.execute_reply":"2023-12-18T09:42:36.799945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"probas = model.predict(X_test)","metadata":{"_cell_guid":"63a95326-bbb5-4513-8a0c-329f87cc1fb9","_execution_state":"busy","_uuid":"80eb60399aedb6f8fcb3b65e215b1340f2b679c2","execution":{"iopub.status.busy":"2023-12-18T09:42:36.801678Z","iopub.status.idle":"2023-12-18T09:42:36.802011Z","shell.execute_reply.started":"2023-12-18T09:42:36.801842Z","shell.execute_reply":"2023-12-18T09:42:36.801858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_indices = np.argmax(probas, axis=1)\nclasses = np.array(range(1, 10))\npreds = classes[pred_indices]\nprint('Log loss: {}'.format(log_loss(classes[np.argmax(Y_test, axis=1)], probas)))\nprint('Accuracy: {}'.format(accuracy_score(classes[np.argmax(Y_test, axis=1)], preds)))\nskplt.plot_confusion_matrix(classes[np.argmax(Y_test, axis=1)], preds)\n","metadata":{"_cell_guid":"da8194c1-bce5-4d3d-93a7-fbb6b832113e","_execution_state":"busy","_uuid":"fffe9f6d9907deabbf827f183c4cf33af917b131","execution":{"iopub.status.busy":"2023-12-18T09:42:36.802962Z","iopub.status.idle":"2023-12-18T09:42:36.803314Z","shell.execute_reply.started":"2023-12-18T09:42:36.803153Z","shell.execute_reply":"2023-12-18T09:42:36.803169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### The results of the quick LSTM are promising. \n\nOn the first try with no hyperparameter search, 6th epoch, max sequence length cut down to a measly 2000 (longest text has 77000+ words), we get the best log loss so far of around 1.4. You can still see a bit of bias towards class 7 but the confusion matrix looks more balanced than anything we've seen so far.\n\n### Further tuning of the LSTM will likely produce better results.","metadata":{"_cell_guid":"e96c819f-eea0-4079-8309-0b5233fb4a1c","_uuid":"0fda6b5af213a4250ba1589b7203ffde3aaead6f"}},{"cell_type":"markdown","source":"## So far, we've only used the text field to perform classification. But there is still the \"Gene\" and \"Variation\" fields.\n\nUsing only the Text field is a bit flawed. Looking closer at the statistics we calculated above, \"training_text\" actually has duplicates, and the duplicates have different classes. This is part of the challenge. A lot of papers are studies of 2 or more genes. It is our job to use the other fields to figure out which parts of the text are relevant for the particular Gene and Variation.\n\n### Let's use a LabelEncoder to encode Gene and Variation and combine it with TFIDF","metadata":{"_cell_guid":"db48f4cc-a354-4f8d-9e60-a9d4462876dd","_uuid":"a3689c71d2b898f2109bef73b38247999535486d"}},{"cell_type":"code","source":"gene_le = LabelEncoder()\ngene_encoded = gene_le.fit_transform(df_train['Gene'].values.ravel()).reshape(-1, 1)\ngene_encoded = gene_encoded / np.max(gene_encoded)","metadata":{"_cell_guid":"704fce25-86e1-48b6-8674-9843559ace23","_execution_state":"busy","_uuid":"80844cb882cf78f1e3e9705f14c3332a4778be48","execution":{"iopub.status.busy":"2023-12-18T09:42:36.804561Z","iopub.status.idle":"2023-12-18T09:42:36.804885Z","shell.execute_reply.started":"2023-12-18T09:42:36.804725Z","shell.execute_reply":"2023-12-18T09:42:36.804741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"variation_le = LabelEncoder()\nvariation_encoded = variation_le.fit_transform(df_train['Variation'].values.ravel()).reshape(-1, 1)\nvariation_encoded = variation_encoded / np.max(variation_encoded)","metadata":{"_cell_guid":"1ceed29c-6f13-41e7-9c85-47f39820e319","_execution_state":"busy","_uuid":"bf24d496955e15ff12bc583b97ed034b57adfd89","execution":{"iopub.status.busy":"2023-12-18T09:42:36.805909Z","iopub.status.idle":"2023-12-18T09:42:36.806251Z","shell.execute_reply.started":"2023-12-18T09:42:36.806090Z","shell.execute_reply":"2023-12-18T09:42:36.806105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_features(np.hstack((gene_encoded, variation_encoded, truncated_tfidf)), df_train['Class'])","metadata":{"_cell_guid":"8dbafc18-a824-4eb5-b8b1-c104e4b14480","_execution_state":"busy","_uuid":"8e98d909ca15dc9e1c756610df216f22c054d8d1","execution":{"iopub.status.busy":"2023-12-18T09:42:36.807270Z","iopub.status.idle":"2023-12-18T09:42:36.807595Z","shell.execute_reply.started":"2023-12-18T09:42:36.807436Z","shell.execute_reply":"2023-12-18T09:42:36.807452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_features(np.hstack((gene_encoded, variation_encoded, truncated_tfidf)), df_train['Class'],\n                  RandomForestClassifier(n_estimators=1000, max_depth=5, verbose=1))","metadata":{"_cell_guid":"14b8a51e-ab20-4791-9fc0-9d1681263f21","_execution_state":"busy","_uuid":"0d151795f1e0a4a5cbf42d740d3d97882c6d970d","execution":{"iopub.status.busy":"2023-12-18T09:42:36.808622Z","iopub.status.idle":"2023-12-18T09:42:36.808941Z","shell.execute_reply.started":"2023-12-18T09:42:36.808782Z","shell.execute_reply":"2023-12-18T09:42:36.808797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Barely any difference, let's try our  label encoded features with our word2vec features","metadata":{"_cell_guid":"772a5a7a-c978-4ce5-bfaf-0e50584b225a","_uuid":"9fdfeaf538aaea6e557a95233a60d7c01def5814"}},{"cell_type":"code","source":"evaluate_features(np.hstack((gene_encoded, variation_encoded, mean_embedded)), df_train['Class'])","metadata":{"_cell_guid":"58ef00cc-e255-4eac-9a56-7ac0406b8230","_execution_state":"busy","_uuid":"e3813a8343a90bd7023055150cc7b9012885247c","execution":{"iopub.status.busy":"2023-12-18T09:42:36.809824Z","iopub.status.idle":"2023-12-18T09:42:36.810210Z","shell.execute_reply.started":"2023-12-18T09:42:36.810015Z","shell.execute_reply":"2023-12-18T09:42:36.810047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_features(np.hstack((gene_encoded, variation_encoded, mean_embedded)), df_train['Class'],\n                  RandomForestClassifier(n_estimators=1000, max_depth=5, verbose=1))","metadata":{"_cell_guid":"43c00792-b741-45de-9709-4c3ad9a7eafc","_execution_state":"busy","_uuid":"fbcc0adde6907f7beab0de93debe310815a1156b","execution":{"iopub.status.busy":"2023-12-18T09:42:36.811845Z","iopub.status.idle":"2023-12-18T09:42:36.812186Z","shell.execute_reply.started":"2023-12-18T09:42:36.812000Z","shell.execute_reply":"2023-12-18T09:42:36.812014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Doesn't make a difference either. Let's try one-hot encoding + SVD the \"Gene\" and \"Variation\" features","metadata":{"_cell_guid":"c1780db6-c523-4a23-a5dd-84906fb0505a","_uuid":"04b528db030e8e16cf0c581643bd02d95923522c"}},{"cell_type":"code","source":"one_hot_gene = pd.get_dummies(df_train['Gene'])\nsvd = TruncatedSVD(n_components=25, n_iter=25, random_state=12)\ntruncated_one_hot_gene = svd.fit_transform(one_hot_gene.values)","metadata":{"_cell_guid":"83c49101-1cbb-492e-9680-ad5f5a4995d2","_execution_state":"busy","_uuid":"343d08a18abfaa87e67064faf592ddbb2d92de2f","execution":{"iopub.status.busy":"2023-12-18T09:42:36.813734Z","iopub.status.idle":"2023-12-18T09:42:36.814219Z","shell.execute_reply.started":"2023-12-18T09:42:36.813952Z","shell.execute_reply":"2023-12-18T09:42:36.813974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"one_hot_variation = pd.get_dummies(df_train['Variation'])\nsvd = TruncatedSVD(n_components=25, n_iter=25, random_state=12)\ntruncated_one_hot_variation = svd.fit_transform(one_hot_variation.values)","metadata":{"_cell_guid":"0340ce68-271b-43da-92e0-a9d22528bb30","_execution_state":"busy","_uuid":"ab2bb2dadc7ee52b5b4e382bad56ba314f93ec7a","execution":{"iopub.status.busy":"2023-12-18T09:42:36.815512Z","iopub.status.idle":"2023-12-18T09:42:36.815833Z","shell.execute_reply.started":"2023-12-18T09:42:36.815675Z","shell.execute_reply":"2023-12-18T09:42:36.815691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Truncated one hot encoding + TFIDF","metadata":{"_cell_guid":"8cb7e817-d34f-4d46-ab04-b33c445bb72a","_uuid":"c4e3b7ae95ead4d81a5bc329a4df5b1016f47b2b"}},{"cell_type":"code","source":"evaluate_features(np.hstack((truncated_one_hot_gene, truncated_one_hot_variation, truncated_tfidf)), df_train['Class'])","metadata":{"_cell_guid":"3c6e9f63-b095-437a-adde-325f86bd58f2","_execution_state":"busy","_uuid":"94704cd2cc00bd6f33a9a2228b01392430806d48","execution":{"iopub.status.busy":"2023-12-18T09:42:36.817404Z","iopub.status.idle":"2023-12-18T09:42:36.817848Z","shell.execute_reply.started":"2023-12-18T09:42:36.817625Z","shell.execute_reply":"2023-12-18T09:42:36.817646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_features(np.hstack((truncated_one_hot_gene, truncated_one_hot_variation, truncated_tfidf)), df_train['Class'],\n                  RandomForestClassifier(n_estimators=1000, max_depth=5, verbose=1))\n","metadata":{"_cell_guid":"69ae841b-f616-468d-980a-bbca4eb40160","_execution_state":"busy","_uuid":"0bc53506063033a3c991226344fc72538c8c6cc3","execution":{"iopub.status.busy":"2023-12-18T09:42:36.819353Z","iopub.status.idle":"2023-12-18T09:42:36.819721Z","shell.execute_reply.started":"2023-12-18T09:42:36.819542Z","shell.execute_reply":"2023-12-18T09:42:36.819559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Truncated one hot encoding + word2vec","metadata":{"_cell_guid":"97164561-dca4-43fe-b37b-3a54d64dd99b","_uuid":"d65b9d9324de3990985b1cc6e6b3b50bb6ad9ca2"}},{"cell_type":"code","source":"evaluate_features(np.hstack((truncated_one_hot_gene, truncated_one_hot_variation, mean_embedded)), df_train['Class'])","metadata":{"_cell_guid":"0e9710c6-8ad0-4462-ae86-71c09306c971","_execution_state":"busy","_uuid":"d8dc844e566435a95016f7dde0087a76ccc94656","execution":{"iopub.status.busy":"2023-12-18T09:42:36.820591Z","iopub.status.idle":"2023-12-18T09:42:36.820906Z","shell.execute_reply.started":"2023-12-18T09:42:36.820748Z","shell.execute_reply":"2023-12-18T09:42:36.820764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_features(np.hstack((truncated_one_hot_gene, truncated_one_hot_variation, mean_embedded)), df_train['Class'],\n                  RandomForestClassifier(n_estimators=1000, max_depth=5, verbose=1))","metadata":{"_cell_guid":"c5b4c744-3ff8-4d96-b80f-358a2a102155","_execution_state":"busy","_uuid":"d684bc32a398a92a77f3b7617dade9340dafd92e","execution":{"iopub.status.busy":"2023-12-18T09:42:36.822026Z","iopub.status.idle":"2023-12-18T09:42:36.822360Z","shell.execute_reply.started":"2023-12-18T09:42:36.822205Z","shell.execute_reply":"2023-12-18T09:42:36.822221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Interestingly, performance is actually a bit worse than simple label encoding","metadata":{"_cell_guid":"a69600f7-640f-4238-8484-c8c0ac40c54c","_uuid":"c238870079622900d50629bcbf0f06914a1c7a87"}},{"cell_type":"markdown","source":"## Before going into a summary of the insights we've discovered, let's generate some submissions from our best models and see how they fare in the public leaderboard\n\n### We'll start by generating a submission from our word2vec model","metadata":{"_cell_guid":"e70c3e0c-3722-4ed0-943f-3909c2078f38","_uuid":"2eeb0f102a078b433d0c30b9d4e5fedebc58a236"}},{"cell_type":"code","source":"lr_w2vec = LogisticRegression()\nlr_w2vec.fit(mean_embedded, df_train['Class'])","metadata":{"_cell_guid":"359545db-061c-428c-9863-77cc73edb2a5","_execution_state":"busy","_uuid":"960016c34ae2a8cc76c9624ee721e712e3e3baf5","execution":{"iopub.status.busy":"2023-12-18T09:42:36.823746Z","iopub.status.idle":"2023-12-18T09:42:36.824156Z","shell.execute_reply.started":"2023-12-18T09:42:36.823934Z","shell.execute_reply":"2023-12-18T09:42:36.823953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mean_embedded_test = mean_embedding_vectorizer.transform(df_test['Text'])","metadata":{"_cell_guid":"b18da40c-43a0-4f3d-b2fd-65d4b5ffe13c","_execution_state":"busy","_uuid":"15f8696fff30dc4377efae0ebca1f47c5470df29","execution":{"iopub.status.busy":"2023-12-18T09:42:36.825911Z","iopub.status.idle":"2023-12-18T09:42:36.826270Z","shell.execute_reply.started":"2023-12-18T09:42:36.826097Z","shell.execute_reply":"2023-12-18T09:42:36.826114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"probas = lr_w2vec.predict_proba(mean_embedded_test)","metadata":{"_cell_guid":"29ef4844-4cc2-4b19-adfc-dfbdd5eb6821","_execution_state":"busy","_uuid":"32c10bef6437d50261d22c3f5fd61fdab46a399f","execution":{"iopub.status.busy":"2023-12-18T09:42:36.827849Z","iopub.status.idle":"2023-12-18T09:42:36.828234Z","shell.execute_reply.started":"2023-12-18T09:42:36.828012Z","shell.execute_reply":"2023-12-18T09:42:36.828027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df = pd.DataFrame(probas, columns=['class'+str(c+1) for c in range(9)])\nsubmission_df['ID'] = df_test['ID']\nsubmission_df.head()","metadata":{"_cell_guid":"85f25966-0e5a-421a-a77c-5a8d75410917","_execution_state":"busy","_uuid":"a106e10bd7e26e6284e583a1411a66c8f1f073e5","execution":{"iopub.status.busy":"2023-12-18T09:42:36.839649Z","iopub.status.idle":"2023-12-18T09:42:36.839977Z","shell.execute_reply.started":"2023-12-18T09:42:36.839815Z","shell.execute_reply":"2023-12-18T09:42:36.839830Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df.to_csv('submission.csv', index=False)","metadata":{"_cell_guid":"9c0da856-df5c-44b6-a8ab-50ce90212974","_execution_state":"busy","_uuid":"f5b95449759c0052a634b624d0c5d18177aa5329","execution":{"iopub.status.busy":"2023-12-18T09:42:36.841141Z","iopub.status.idle":"2023-12-18T09:42:36.841488Z","shell.execute_reply.started":"2023-12-18T09:42:36.841318Z","shell.execute_reply":"2023-12-18T09:42:36.841334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Test out XGB and SVC","metadata":{"_cell_guid":"d30f0cd2-bd38-4913-941f-2de07416dce9","_uuid":"2313e8a97d9801826e7499fafbcf73abe353df28"}},{"cell_type":"code","source":"xgb_w2vec = XGBClassifier(max_depth=4,\n                          objective='multi:softprob',\n                          learning_rate=0.03333)\nxgb_w2vec.fit(mean_embedded, df_train['Class'])\nprobas = xgb_w2vec.predict_proba(mean_embedded_test)\nsubmission_df = pd.DataFrame(probas, columns=['class'+str(c+1) for c in range(9)])\nsubmission_df['ID'] = df_test['ID']\nsubmission_df.to_csv('submission.csv', index=False)","metadata":{"_cell_guid":"366147f8-7898-4c43-8e7d-3bb9092c0914","_execution_state":"busy","_uuid":"936695ec044ae89260c3e191b9af9a1c84e21e67","execution":{"iopub.status.busy":"2023-12-18T09:42:36.843373Z","iopub.status.idle":"2023-12-18T09:42:36.843757Z","shell.execute_reply.started":"2023-12-18T09:42:36.843574Z","shell.execute_reply":"2023-12-18T09:42:36.843592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svc_w2vec = SVC(kernel='linear', probability=True)\nsvc_w2vec.fit(mean_embedded, df_train['Class'])\nprobas = svc_w2vec.predict_proba(mean_embedded_test)\nsubmission_df = pd.DataFrame(probas, columns=['class'+str(c+1) for c in range(9)])\nsubmission_df['ID'] = df_test['ID']\nsubmission_df.to_csv('submission.csv', index=False)","metadata":{"_cell_guid":"c313def8-9b81-4257-ae96-f826130c18aa","_execution_state":"busy","_uuid":"4dffea7a53a0953e51fe6ab353dcc24c45ec7e4a","execution":{"iopub.status.busy":"2023-12-18T09:42:36.844865Z","iopub.status.idle":"2023-12-18T09:42:36.845251Z","shell.execute_reply.started":"2023-12-18T09:42:36.845069Z","shell.execute_reply":"2023-12-18T09:42:36.845091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Public LB Score Log Reg: 1.032000\n\n#### Public LB Score XGB: 0.96536\n\n#### Public LB Score SVC: 0.97059\n\n### Let's try our Keras model","metadata":{"_cell_guid":"8cfef508-e869-4971-9b84-06d3e80ae6ea","_uuid":"9c2e087dd457065520e8e8d1d195dc015571925b"}},{"cell_type":"code","source":"Xtest = tokenizer.texts_to_sequences(df_test['Text'].values)\nXtest = pad_sequences(Xtest, maxlen=2000)","metadata":{"_cell_guid":"56db5d32-cc80-445f-b33a-05071832dd69","_execution_state":"busy","_uuid":"8d65e2e32ad9d552e5f38c3667f15bfdb9915208","execution":{"iopub.status.busy":"2023-12-18T09:42:36.846769Z","iopub.status.idle":"2023-12-18T09:42:36.847160Z","shell.execute_reply.started":"2023-12-18T09:42:36.846946Z","shell.execute_reply":"2023-12-18T09:42:36.846962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"probas = model.predict(Xtest)","metadata":{"_cell_guid":"4d608be7-741c-462c-8a2a-39b81612000d","_execution_state":"busy","_uuid":"7b454ebc7996841cffcaba3a165d8b28b5c76466","execution":{"iopub.status.busy":"2023-12-18T09:42:36.849496Z","iopub.status.idle":"2023-12-18T09:42:36.849965Z","shell.execute_reply.started":"2023-12-18T09:42:36.849721Z","shell.execute_reply":"2023-12-18T09:42:36.849743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df = pd.DataFrame(probas, columns=['class'+str(c+1) for c in range(9)])\nsubmission_df['ID'] = df_test['ID']\nsubmission_df.head()","metadata":{"_cell_guid":"dbcb71f9-1854-44c3-b9cf-d060494887d8","_execution_state":"busy","_uuid":"81165d6e9870765c90fbb4624743ea49f9f6d1d8","execution":{"iopub.status.busy":"2023-12-18T09:42:36.851217Z","iopub.status.idle":"2023-12-18T09:42:36.851562Z","shell.execute_reply.started":"2023-12-18T09:42:36.851394Z","shell.execute_reply":"2023-12-18T09:42:36.851411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df.to_csv('submission.csv', index=False)","metadata":{"_cell_guid":"9ce11af2-21b8-47e0-a83b-55ee14c1c53c","_execution_state":"busy","_uuid":"7264bb21f9c0028b6be3e637c8414d543266d113","execution":{"iopub.status.busy":"2023-12-18T09:42:36.852996Z","iopub.status.idle":"2023-12-18T09:42:36.853357Z","shell.execute_reply.started":"2023-12-18T09:42:36.853193Z","shell.execute_reply":"2023-12-18T09:42:36.853210Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Public LB Score: 1.00234","metadata":{"_cell_guid":"d1aa03cb-3faf-4dfe-97a7-97dbfff856ec","_uuid":"1ac56c47c3e55b3a6fde88332fe1e045e59fef9b"}},{"cell_type":"markdown","source":"## Summary\n\nThe aim of this notebook was to do some quick exploration of the dataset and apply some common ML techniques to the classification task. The metric to maximize is multiclass log loss.\n\nA big part of the problem is to teach an ML model how to \"read\" medical literature and classify the given Gene and Variation into 1 out of 9 classes.\n\nThus, the first part of this notebook focused on applying common techniques to preprocess and vectorize free text and evaluate its effectiveness by running them through vanilla Logistic Regression and Random Forest.\n\nThe techniques used, from least effective to most effective, were:\n\n* Bag of Words\n* TF-IDF\n* Word2Vec\n\nBecause the above approaches did not take into account the temporal patterns in free text, a quick LSTM was tried as well. This approach scored higher than the above without any tuning.\n\nIn the second part of the notebook, I added the \"Gene\" and \"Variation\" features next to the free text features. I tried both label encoding and one-hot encoding, however, the results did not show much improvement.\n\nIn the third part of the notebook, I generated submissions for both Word2Vec (multiple classifiers) and Keras LSTM and recorded the public leaderboard scores of each submission. The scores were better (around 1) but did not show the same relationships with each other as my own CV (they were mostly close to each other). This is a common occurrence in Kaggle competitions since the public leaderboard is scored on a smaller subset of the test data. Most Kagglers' advice is to ignore the public leaderboard and trust your own CV.\n\n## Further things to try\n\nThis notebook's aim was mostly figuring out which techniques are worth exploring and was not intended to generate very competitive submissions. The following is a list of suggestions to try for further improvement.\n\n* There are tons of other techniques for free text other than the ones I listed above. Make sure to explore other techniques such as Doc2Vec, DeepIR, and Word Mover's distance\n\n* Focus more on capturing the relationship between \"Gene\" and \"Variation\" with the free text features. Since \"Text\" is sometimes duplicated (with different classes!), taking into account \"Gene\" and \"Variation\" is very important.\n\n* Explore different deep learning architectures for the data. One idea for an architecture is to combine a simple Embedding + LSTM for the free text and concatenate the input with \"Gene\" and \"Variation\" Embeddings, leading into a final fully connected layer for the classes. Hopefully, this will capture the relationship between the text and the \"Gene\" + \"Variation\" columns.\n\n* Train Word2Vec on a bigger corpus of genetic and medical data. Since Word2Vec is unsupervised, we can get better embeddings with more data, and consequently, better predictions\n\n* Don't forget to do hyperparameter optimization when you're happy with a set of features. Stacked ensembling is also an almost guaranteed way to get a small boost to your score. We skipped this entirely in this notebook as this is usually the last step in the process. Try http://xcessiv.readthedocs.io/.\n\n","metadata":{"_cell_guid":"04e8cc4c-7bcd-4951-8e8f-859a6da7e0fe","_uuid":"9070256ed93261c08d7ff3c7bf43f144463263b5"}},{"cell_type":"code","source":"","metadata":{"_cell_guid":"47a36edb-f857-4de7-a19d-99dbb68aab50","_execution_state":"busy","_uuid":"609fdc82b3b3125e3f79ea88909dfab79484b829","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_cell_guid":"1e994ded-dbfe-4117-a034-4ea3b57be80f","_execution_state":"busy","_uuid":"a8e5d0e66cf98fc72ec808b7f5e0e366d4a492b2","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_cell_guid":"1bcacfa6-87a4-489c-aa38-5a7f88424134","_execution_state":"busy","_uuid":"7ee8525ea2ffedb4c8d76afeba30598f5589b2f4","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]}]}